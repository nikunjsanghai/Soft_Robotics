{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "loIShgzj2wO9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "device = torch.device('cuda:' + str(gpu) if torch.cuda.is_available() else 'cpu')\n",
        "# Print whether CPU or GPU is used.\n",
        "device = torch.device(\"cuda:0\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Populate the Data for m, k1, k2, x, and y**"
      ],
      "metadata": {
        "id": "dzvcv4of-5pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = np.random.uniform(0,1,(1,100000)) \n",
        "k1 = np.random.uniform(0,1,(1,100000))\n",
        "k2 = k1\n",
        "g=9.81\n",
        "term = m * g / (k1 + k2)\n",
        "posy =  np.sqrt( term ** 2 + term)# fill in the blank\n",
        "posx =  0.5 * np.ones((1, 100000))# fill in the blank"
      ],
      "metadata": {
        "id": "hc5dZtWc29Pe"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m_test = np.random.uniform(0,1,(1,100000))\n",
        "k1_test = np.random.uniform(0,1,(1,100000))\n",
        "k2_test = k1_test\n",
        "g = 9.81\n",
        "term = m * g / (k1_test + k2_test)\n",
        "posy_test = np.sqrt( term ** 2 + term)  # fill in the blank\n",
        "posx_test = 0.5 * np.ones((1, 100000)) # fill in the blank"
      ],
      "metadata": {
        "id": "OAlgf7ytAsTH"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Data(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.from_numpy(X.astype(np.float32))\n",
        "        self.y = torch.from_numpy(y.astype(np.float32))\n",
        "        self.len = self.X.shape[0]\n",
        "       \n",
        "    def __getitem__(self, index):\n",
        "        return self.X[index], self.y[index]\n",
        "   \n",
        "    def __len__(self):\n",
        "        return self.len"
      ],
      "metadata": {
        "id": "lTERA7X3_Ig-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1000 #Choose your own batch size"
      ],
      "metadata": {
        "id": "J4zhEBtC-qsN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_input = np.column_stack(np.concatenate([k1, k2, posx, posy]))  # Fill in the input data format size : ()\n",
        "data_output = np.transpose(m) \n",
        "data_input_test = np.column_stack(np.concatenate([k1_test, k2_test, posx_test, posy_test]))   # Fill in the input data format size : ()\n",
        "data_output_test =np.transpose(m_test) "
      ],
      "metadata": {
        "id": "TJLeSxrlAcFb"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = Data(data_input, data_output)\n",
        "train_set = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_data = Data(data_input_test, data_output_test)\n",
        "test_set = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "QQ-K7AAL-1vd"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable\n",
        "input_size = 4 # fill in the blank. \n",
        "hidden_layer_size = 10 # fill in the blank. \n",
        "learning_rate = 0.05 # fill in the blank. \n",
        "num_epochs = 100 # fill in the blank. \n",
        "class RegressionModel(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(RegressionModel, self).__init__()\n",
        "        self.dense_h1 = nn.Linear(in_features=input_size, out_features=hidden_size) \n",
        "        self.relu_h1 = nn.ReLU()# choose your own activation function\n",
        "        self.dense_h2 = nn.Linear(in_features=hidden_size, out_features=hidden_size)  \n",
        "        self.relu_h2 = nn.ReLU() # choose your own activation function\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.dense_out = nn.Linear(in_features=hidden_size, out_features=1)\n",
        "\n",
        "    def forward(self, X):\n",
        "\n",
        "        out = self.relu_h1(self.dense_h1(X))\n",
        "        out = self.relu_h2(self.dense_h2(out))\n",
        "        out = self.dropout(out)\n",
        "        out = self.dense_out(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "m = RegressionModel(input_size=input_size, hidden_size=hidden_layer_size)\n",
        "\n",
        "cost_func = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(m.parameters(), lr=learning_rate) \n",
        "\n",
        "all_losses = []\n",
        "for e in range(num_epochs):\n",
        "    batch_losses = []\n",
        "\n",
        "    for ix, (Xb, yb) in enumerate(train_set):\n",
        "\n",
        "        _X = Variable(Xb).float()\n",
        "        _y = Variable(yb).float()\n",
        "\n",
        "        #==========Forward pass===============\n",
        "\n",
        "        preds = m(_X)\n",
        "        loss = cost_func(preds, _y)\n",
        "\n",
        "        #==========backward pass==============\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_losses.append(loss.data)\n",
        "        all_losses.append(loss.data)\n",
        "\n",
        "    mbl = np.mean(np.sqrt(batch_losses)).round(3)\n",
        "\n",
        "    if e % 5 == 0:\n",
        "        print(\"Epoch [{}/{}], Batch loss: {}\".format(e, num_epochs, mbl))\n",
        "\n",
        "# prepares model for inference when trained with a dropout layer\n",
        "print(m.training)\n",
        "m.eval()\n",
        "print(m.training)\n",
        "\n",
        "test_batch_losses = []\n",
        "for _X, _y in test_set:\n",
        "\n",
        "    _X = Variable(_X).float()\n",
        "    _y = Variable(_y).float()\n",
        "\n",
        "    #apply model\n",
        "    test_preds = m(_X)\n",
        "    test_loss = cost_func(test_preds, _y)\n",
        "\n",
        "    test_batch_losses.append(test_loss.data)\n",
        "    print(\"Batch loss: {}\".format(test_loss.data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uW7-y17kLgTO",
        "outputId": "fbefb27d-cd6d-456c-84e4-8ad702d51c7c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0/100], Batch loss: 6.22599983215332\n",
            "Epoch [5/100], Batch loss: 0.24799999594688416\n",
            "Epoch [10/100], Batch loss: 0.2460000067949295\n",
            "Epoch [15/100], Batch loss: 0.24400000274181366\n",
            "Epoch [20/100], Batch loss: 0.24400000274181366\n",
            "Epoch [25/100], Batch loss: 0.24400000274181366\n",
            "Epoch [30/100], Batch loss: 0.24400000274181366\n",
            "Epoch [35/100], Batch loss: 0.24400000274181366\n",
            "Epoch [40/100], Batch loss: 0.24300000071525574\n",
            "Epoch [45/100], Batch loss: 0.24199999868869781\n",
            "Epoch [50/100], Batch loss: 0.24300000071525574\n",
            "Epoch [55/100], Batch loss: 0.24300000071525574\n",
            "Epoch [60/100], Batch loss: 0.24300000071525574\n",
            "Epoch [65/100], Batch loss: 0.24199999868869781\n",
            "Epoch [70/100], Batch loss: 0.24300000071525574\n",
            "Epoch [75/100], Batch loss: 0.24300000071525574\n",
            "Epoch [80/100], Batch loss: 0.24300000071525574\n",
            "Epoch [85/100], Batch loss: 0.24300000071525574\n",
            "Epoch [90/100], Batch loss: 0.24199999868869781\n",
            "Epoch [95/100], Batch loss: 0.24300000071525574\n",
            "True\n",
            "False\n",
            "Batch loss: 0.09337511658668518\n",
            "Batch loss: 0.09286325424909592\n",
            "Batch loss: 0.09400773793458939\n",
            "Batch loss: 0.09131795167922974\n",
            "Batch loss: 0.09340649098157883\n",
            "Batch loss: 0.09358077496290207\n",
            "Batch loss: 0.08650439977645874\n",
            "Batch loss: 0.09229452908039093\n",
            "Batch loss: 0.09236500412225723\n",
            "Batch loss: 0.09103856980800629\n",
            "Batch loss: 0.091115303337574\n",
            "Batch loss: 0.08982226252555847\n",
            "Batch loss: 0.09408006072044373\n",
            "Batch loss: 0.0947510376572609\n",
            "Batch loss: 0.09607262909412384\n",
            "Batch loss: 0.09064274281263351\n",
            "Batch loss: 0.09007338434457779\n",
            "Batch loss: 0.092770516872406\n",
            "Batch loss: 0.08781381696462631\n",
            "Batch loss: 0.09190771728754044\n",
            "Batch loss: 0.09307824820280075\n",
            "Batch loss: 0.09684807062149048\n",
            "Batch loss: 0.09131398797035217\n",
            "Batch loss: 0.08848496526479721\n",
            "Batch loss: 0.09347039461135864\n",
            "Batch loss: 0.09012433141469955\n",
            "Batch loss: 0.09016454219818115\n",
            "Batch loss: 0.09473828971385956\n",
            "Batch loss: 0.09515687078237534\n",
            "Batch loss: 0.09600807726383209\n",
            "Batch loss: 0.09311792254447937\n",
            "Batch loss: 0.09873110800981522\n",
            "Batch loss: 0.09269872307777405\n",
            "Batch loss: 0.09281258285045624\n",
            "Batch loss: 0.0879557728767395\n",
            "Batch loss: 0.08842666447162628\n",
            "Batch loss: 0.09195210784673691\n",
            "Batch loss: 0.09462598711252213\n",
            "Batch loss: 0.09026113897562027\n",
            "Batch loss: 0.09446762502193451\n",
            "Batch loss: 0.0953199565410614\n",
            "Batch loss: 0.08668483793735504\n",
            "Batch loss: 0.08713432401418686\n",
            "Batch loss: 0.08868585526943207\n",
            "Batch loss: 0.09441572427749634\n",
            "Batch loss: 0.09410806745290756\n",
            "Batch loss: 0.09348025918006897\n",
            "Batch loss: 0.08716963231563568\n",
            "Batch loss: 0.09591232985258102\n",
            "Batch loss: 0.09011724591255188\n",
            "Batch loss: 0.09124024957418442\n",
            "Batch loss: 0.0913270115852356\n",
            "Batch loss: 0.0955619290471077\n",
            "Batch loss: 0.09480373561382294\n",
            "Batch loss: 0.09029338508844376\n",
            "Batch loss: 0.09022238105535507\n",
            "Batch loss: 0.0874217078089714\n",
            "Batch loss: 0.09381631016731262\n",
            "Batch loss: 0.09016812592744827\n",
            "Batch loss: 0.08953287452459335\n",
            "Batch loss: 0.08905132114887238\n",
            "Batch loss: 0.09070637077093124\n",
            "Batch loss: 0.09464843571186066\n",
            "Batch loss: 0.09560751169919968\n",
            "Batch loss: 0.09241706132888794\n",
            "Batch loss: 0.09407656639814377\n",
            "Batch loss: 0.09032361954450607\n",
            "Batch loss: 0.09192914515733719\n",
            "Batch loss: 0.08883151412010193\n",
            "Batch loss: 0.09855609387159348\n",
            "Batch loss: 0.09298697113990784\n",
            "Batch loss: 0.09189965575933456\n",
            "Batch loss: 0.09265367686748505\n",
            "Batch loss: 0.08962894231081009\n",
            "Batch loss: 0.09075074642896652\n",
            "Batch loss: 0.094632588326931\n",
            "Batch loss: 0.09207002073526382\n",
            "Batch loss: 0.09252536296844482\n",
            "Batch loss: 0.0923875942826271\n",
            "Batch loss: 0.09359069913625717\n",
            "Batch loss: 0.09390507638454437\n",
            "Batch loss: 0.08844060450792313\n",
            "Batch loss: 0.09449189901351929\n",
            "Batch loss: 0.09220008552074432\n",
            "Batch loss: 0.0922950878739357\n",
            "Batch loss: 0.09365257620811462\n",
            "Batch loss: 0.08905018121004105\n",
            "Batch loss: 0.09040748327970505\n",
            "Batch loss: 0.09139005094766617\n",
            "Batch loss: 0.08711037784814835\n",
            "Batch loss: 0.08837258815765381\n",
            "Batch loss: 0.0973716527223587\n",
            "Batch loss: 0.0943681076169014\n",
            "Batch loss: 0.09382355213165283\n",
            "Batch loss: 0.0940910056233406\n",
            "Batch loss: 0.09374446421861649\n",
            "Batch loss: 0.08833325654268265\n",
            "Batch loss: 0.09370921552181244\n",
            "Batch loss: 0.09081573784351349\n",
            "Batch loss: 0.09281843155622482\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "fig, ax = plt.subplots(figsize=(8,5))\n",
        "plt.plot(np.array(all_losses))\n",
        "plt.title(\"Step-wise Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MDyYW52_8rOm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "1ae9b4ae-2532-4f0b-d4c5-a932ab4d3395"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgcAAAFNCAYAAACUisysAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAf/ElEQVR4nO3de7RedX3n8ffHxCCKkAAphQRMrFEnMlUxRby044iFoI5hTb3AuEq0jIy3FltXFdqZYdXLjLYdsWkVhwoSrAUptSVLUUwBoZ0WJCgCAZUjoCRyCSYQ8AKC3/nj+UWffTgneZKccx5y8n6t9ayz93f/9t6/vdnkfM6+PakqJEmStnjCsDsgSZIeXwwHkiSpw3AgSZI6DAeSJKnDcCBJkjoMB5IkqcNwIGkokrwxyZeH3Q9Jj2U4kKahJC9N8q9J7k+yMcn/S/JrbdqbkvzLsPtYVZ+pqqMmcpmPl22TdnUzh90BSRMryd7A54G3ARcAs4BfBx4aZr8k7To8cyBNP88EqKrzqurRqvpxVX25qq5P8u+ATwAvSvJgkvsAkuyR5M+TfC/J3Uk+kWTPNu1lSdYl+aMk9ya5Pckbx1t5kiuS/FYbfkmSSvKqNn5kkuva8M//yk/P6UnuSbI5yQ1JDt1W37ZHkhcnuaadTbkmyYv7pr0pya1JHkhy25btS/KMtj33t23/7PauV9oVGQ6k6efbwKNJViY5JsmcLROq6mbgrcC/VdVeVTW7TfoQvVDxPOAZwDzgf/Yt85eB/Vt9OXBmkmeNs/4rgJe14f8A3Ar8Rt/4FWPMc1Rr80xgH+D1wA8G7Ns2JdkX+AKwAtgP+AjwhST7JXlKqx9TVU8FXgxc12Z9P/BlYA4wH/jL7VmvtKsyHEjTTFVtBl4KFPDXwIYkq5IcMFb7JAFOAn6/qjZW1QPA/wKOG9X0f1TVQ1V1Bb1ftK8fpwtX0AsB0PuF/7/7xscLBz8Fngo8G0hV3VxVd25H37blVcAtVfXpqnqkqs4Dvgn8pzb9Z8ChSfasqjuram1fv54GHFRVP6kq72fQbsFwIE1D7Zfrm6pqPnAocBDw0XGazwWeDFyb5L52qeFLrb7Fpqr6Yd/4d4GDkhzSLk88mOTBNu3fgGe2MPI84Fzg4CT7A4cDV47R38uAvwI+BtyT5Mx278QgfRvEQa3P/b4LzGvb9QZ6Z1TuTPKFJM9ubd4DBPhqkrVJfmc71yvtkgwH0jRXVd8EzqEXEqB3RqHfvcCPgedU1ez22aeq9uprM6edft/iEOD7VfW9dnliry3tq+pHwLXAycCNVfUw8K/AHwDfqap7x+nniqp6AbCY3mWEPxywb4P4Pr0zAP0OAda3dV9SVb8JHEjvjMJft/pdVfWWqjoI+G/Ax5M8YzvXLe1yDAfSNJPk2UnenWR+Gz8YOB64qjW5G5ifZBZAVf2M3i/D05P8UptnXpKjRy36T5LMSvLrwKuBv9tKN64A3skvLiF8ZdT46D7/WpIXJnki8EPgJ8DPtqNvoxaXJ/V/gIvpnc34L0lmJnkDvRDy+SQHJFnWws9DwIP0LjOQ5HVb9iOwiV6w+tlW1i1NC4YDafp5AHghcHWSH9ILBTcC727TLwPWAncl2fJX/HuBEeCqJJuBfwL6bzi8i94vx+8DnwHe2s5IjOcKevcQXDnO+Gh70wsBm+id7v8B8GcD9m20F9M729D/uZ9eoHl3W/Z7gFe3sxhPoHdW4/vARnr3RbytLevX6O3HB4FVwMlVdetW1i1NC6kafYZRkn4hycuAv2n3L0jaDXjmQJIkdRgOJElSx6SFgyRnt7ed3TjGtHe3t6bt38aTZEWSkSTXJzmsr+3yJLe0z/K++gvaW9RG2rxp9X2TrG7tV/e/AEbS9quqr3hJQdq9TOaZg3OApaOL7c7po4Dv9ZWPARa1z0nAGa3tvsBp9G6uOhw4re+X/RnAW/rm27KuU4BLq2oRcGkblyRJA5q0cFBVV9K783e00+ndKdx/J+Qy4NzquQqYneRA4GhgdXsz2iZgNbC0Tdu7qq6q3h2V5wLH9i1rZRte2VeXJEkDmNJvZUyyDFhfVd9oVwG2mAfc0Te+rtW2Vl83Rh3ggKq6sw3fBYz5ytjR9t9//1qwYMFgGyJJ0i7u2muvvbeqxnzb6JSFgyRPBv6I3iWFKVFVlWTcZzWTnETvMgaHHHIIa9asmaquSZI0VElGv1L856byaYVfARYC30hyO71vOPtakl+m9wrTg/vazm+1rdXnj1EHuLtddqD9vGe8DlXVmVW1pKqWzJ27va9qlyRpepqycFBVN1TVL1XVgqpaQO9SwGFVdRe9N4+d0J5aOAK4v10auAQ4KsmcdiPiUcAlbdrmJEe0pxROAC5qq1pF7ytlaT8vQpIkDWwyH2U8j963sz0rybokJ26l+cX0vvN9hN4rVN8OUFUb6X2f+jXt875Wo7X5ZJvnO8AXW/1DwG8muQV4RRuXJEkD8vXJzZIlS8p7DiRJu4sk11bVkrGm+YZESZLUYTiQJEkdhgNJktRhOJAkSR2GA0mS1GE4mCQ3fX8z9zzwk2F3Q5Kk7WY4mCSvXPHPvOzPvjLsbkiStN0MB5PoRw8/OuwuSJK03QwHkiSpw3AgSZI6DAeSJKnDcCBJkjoMB5IkqcNwIEmSOgwHkiSpw3AgSZI6DAeSJKnDcCBJkjoMB5IkqcNwIEmSOgwHkiSpw3AgSZI6DAeSJKnDcCBJkjoMB5IkqcNwIEmSOgwHkiSpw3AgSZI6Ji0cJDk7yT1Jbuyr/VmSbya5Psk/JJndN+3UJCNJvpXk6L760lYbSXJKX31hkqtb/bNJZrX6Hm18pE1fMFnbKEnSdDSZZw7OAZaOqq0GDq2qXwW+DZwKkGQxcBzwnDbPx5PMSDID+BhwDLAYOL61BfgwcHpVPQPYBJzY6icCm1r99NZOkiQNaNLCQVVdCWwcVftyVT3SRq8C5rfhZcD5VfVQVd0GjACHt89IVd1aVQ8D5wPLkgR4OXBhm38lcGzfsla24QuBI1t7SZI0gGHec/A7wBfb8Dzgjr5p61ptvPp+wH19QWNLvbOsNv3+1l6SJA1gKOEgyR8DjwCfGcb6+/pxUpI1SdZs2LBhmF2RJOlxY8rDQZI3Aa8G3lhV1crrgYP7ms1vtfHqPwBmJ5k5qt5ZVpu+T2v/GFV1ZlUtqaolc+fO3cktkyRpepjScJBkKfAe4DVV9aO+SauA49qTBguBRcBXgWuARe3JhFn0blpc1ULF5cBr2/zLgYv6lrW8Db8WuKwvhEiSpG2Yue0mOybJecDLgP2TrANOo/d0wh7A6naP4FVV9daqWpvkAuAmepcb3lFVj7blvBO4BJgBnF1Va9sq3gucn+QDwNeBs1r9LODTSUbo3RB53GRtoyRJ09GkhYOqOn6M8llj1La0/yDwwTHqFwMXj1G/ld7TDKPrPwFet12dlSRJP+cbEiVJUofhQJIkdRgOJElSh+FAkiR1GA4kSVKH4UCSJHUYDiRJUofhQJIkdRgOJElSh+FAkiR1GA4kSVKH4UCSJHUYDiRJUofhQJIkdRgOJElSh+FAkiR1GA4kSVKH4UCSJHUYDiRJUofhQJIkdRgOJElSh+FAkiR1GA4kSVKH4UCSJHUYDiRJUofhQJIkdRgOJElSh+FAkiR1TFo4SHJ2knuS3NhX2zfJ6iS3tJ9zWj1JViQZSXJ9ksP65lne2t+SZHlf/QVJbmjzrEiSra1DkiQNZjLPHJwDLB1VOwW4tKoWAZe2cYBjgEXtcxJwBvR+0QOnAS8EDgdO6/tlfwbwlr75lm5jHZIkaQCTFg6q6kpg46jyMmBlG14JHNtXP7d6rgJmJzkQOBpYXVUbq2oTsBpY2qbtXVVXVVUB545a1ljrkCRJA5jqew4OqKo72/BdwAFteB5wR1+7da22tfq6MepbW4ckSRrA0G5IbH/x1zDXkeSkJGuSrNmwYcNkdkWSpF3GVIeDu9slAdrPe1p9PXBwX7v5rba1+vwx6ltbx2NU1ZlVtaSqlsydO3eHN0qSpOlkqsPBKmDLEwfLgYv66ie0pxaOAO5vlwYuAY5KMqfdiHgUcEmbtjnJEe0phRNGLWusdUiSpAHMnKwFJzkPeBmwf5J19J46+BBwQZITge8Cr2/NLwZeCYwAPwLeDFBVG5O8H7imtXtfVW25yfHt9J6I2BP4YvuwlXVIkqQBTFo4qKrjx5l05BhtC3jHOMs5Gzh7jPoa4NAx6j8Yax2SJGkwviFRkiR1GA4kSVKH4UCSJHUYDiRJUofhQJIkdRgOJElSh+FAkiR1GA4kSVKH4UCSJHUYDiRJUofhQJIkdRgOJElSh+FAkiR1GA4kSVKH4UCSJHUYDiRJUofhQJIkdRgOJElSh+FAkiR1GA4kSVKH4UCSJHUYDiRJUofhQJIkdRgOJElSh+FAkiR1GA4kSVKH4UCSJHUYDiRJUsdQwkGS30+yNsmNSc5L8qQkC5NcnWQkyWeTzGpt92jjI236gr7lnNrq30pydF99aauNJDll6rdQkqRd15SHgyTzgN8DllTVocAM4Djgw8DpVfUMYBNwYpvlRGBTq5/e2pFkcZvvOcBS4ONJZiSZAXwMOAZYDBzf2kqSpAEM67LCTGDPJDOBJwN3Ai8HLmzTVwLHtuFlbZw2/cgkafXzq+qhqroNGAEOb5+Rqrq1qh4Gzm9tJUnSAKY8HFTVeuDPge/RCwX3A9cC91XVI63ZOmBeG54H3NHmfaS136+/Pmqe8eqSJGkAw7isMIfeX/ILgYOAp9C7LDDlkpyUZE2SNRs2bBhGFyRJetwZxmWFVwC3VdWGqvop8DngJcDsdpkBYD6wvg2vBw4GaNP3AX7QXx81z3j1x6iqM6tqSVUtmTt37kRsmyRJu7xhhIPvAUckeXK7d+BI4CbgcuC1rc1y4KI2vKqN06ZfVlXV6se1pxkWAouArwLXAIva0w+z6N20uGoKtkuSpGlh5rabTKyqujrJhcDXgEeArwNnAl8Azk/ygVY7q81yFvDpJCPARnq/7KmqtUkuoBcsHgHeUVWPAiR5J3AJvSchzq6qtVO1fZIk7eqmPBwAVNVpwGmjyrfSe9JgdNufAK8bZzkfBD44Rv1i4OKd76kkSbsf35AoSZI6DAeSJKnDcCBJkjoMB5IkqcNwIEmSOgwHkiSpw3AgSZI6DAeSJKnDcCBJkjoMB5IkqWOgcJDkKUme0IafmeQ1SZ44uV2TJEnDMOiZgyuBJyWZB3wZ+G3gnMnqlCRJGp5Bw0Gq6kfAfwY+XlWvA54zed2SJEnDMnA4SPIi4I30vloZel+HLEmSpplBw8G7gFOBf6iqtUmeDlw+ed2SJEnDMnOQRlV1BXAFQLsx8d6q+r3J7JgkSRqOQZ9W+Nskeyd5CnAjcFOSP5zcrkmSpGEY9LLC4qraDBwLfBFYSO+JBUmSNM0MGg6e2N5rcCywqqp+CtTkdUuSJA3LoOHg/wK3A08BrkzyNGDzZHVKkiQNz6A3JK4AVvSVvpvkP05OlyRJ0jANekPiPkk+kmRN+/wfemcRJEnSNDPoZYWzgQeA17fPZuBTk9UpSZI0PANdVgB+pap+q2/8T5JcNxkdkiRJwzXomYMfJ3nplpEkLwF+PDldkiRJwzTomYO3Aucm2aeNbwKWT06XJEnSMA36tMI3gOcm2buNb07yLuD6yeycJEmaeoNeVgB6oaC9KRHgDyahP5Ikaci2KxyMkh2eMZmd5MIk30xyc5IXJdk3yeokt7Sfc1rbJFmRZCTJ9UkO61vO8tb+liTL++ovSHJDm2dFkh3uqyRJu5udCQc78/rkvwC+VFXPBp4L3AycAlxaVYuAS9s4wDHAovY5CTgDIMm+wGnAC4HDgdO2BIrW5i198y3dib5KkrRb2Wo4SPJAks1jfB4ADtqRFbabGn8DOAugqh6uqvuAZcDK1mwlve9xoNXPrZ6rgNlJDgSOBlZX1caq2gSsBpa2aXtX1VVVVcC5fcuSJEnbsNUbEqvqqZOwzoXABuBTSZ4LXAucDBxQVXe2NncBB7ThecAdffOva7Wt1deNUZckSQPYmcsKO2omcBhwRlU9H/ghv7iEAED7i3/Sv/UxyUlbXgm9YcOGyV6dJEm7hGGEg3XAuqq6uo1fSC8s3N0uCdB+3tOmrwcO7pt/fqttrT5/jPpjVNWZVbWkqpbMnTt3pzZKkqTpYsrDQVXdBdyR5FmtdCRwE7CKX7xYaTlwURteBZzQnlo4Ari/XX64BDgqyZx2I+JRwCVt2uYkR7SnFE7oW5YkSdqGQd+QONF+F/hMklnArcCb6QWVC5KcCHyX3hc8AVwMvBIYAX7U2lJVG5O8H7imtXtfVW1sw28HzgH2BL7YPpIkaQBDCQdVdR2wZIxJR47RtoB3jLOcs+l9Y+To+hrg0J3spiRJu6Vh3HMgSZIexwwHkiSpw3AgSZI6DAeSJKnDcCBJkjoMB5IkqcNwIEmSOgwHkiSpw3AgSZI6DAeSJKnDcCBJkjoMB5IkqcNwIEmSOgwHkiSpw3AgSZI6DAeSJKnDcCBJkjoMB5IkqcNwIEmSOgwHkiSpw3AgSZI6DAeSJKnDcCBJkjoMB5IkqcNwIEmSOgwHkiSpw3AgSZI6DAeSJKljaOEgyYwkX0/y+Ta+MMnVSUaSfDbJrFbfo42PtOkL+pZxaqt/K8nRffWlrTaS5JSp3jZJknZlwzxzcDJwc9/4h4HTq+oZwCbgxFY/EdjU6qe3diRZDBwHPAdYCny8BY4ZwMeAY4DFwPGtrSRJGsBQwkGS+cCrgE+28QAvBy5sTVYCx7bhZW2cNv3I1n4ZcH5VPVRVtwEjwOHtM1JVt1bVw8D5ra0kSRrAsM4cfBR4D/CzNr4fcF9VPdLG1wHz2vA84A6ANv3+1v7n9VHzjFeXJEkDmPJwkOTVwD1Vde1Ur3uMvpyUZE2SNRs2bBh2dyRJelwYxpmDlwCvSXI7vVP+Lwf+ApidZGZrMx9Y34bXAwcDtOn7AD/or4+aZ7z6Y1TVmVW1pKqWzJ07d+e3TJKkaWDKw0FVnVpV86tqAb0bCi+rqjcClwOvbc2WAxe14VVtnDb9sqqqVj+uPc2wEFgEfBW4BljUnn6Y1daxago2TZKkaWHmtptMmfcC5yf5APB14KxWPwv4dJIRYCO9X/ZU1dokFwA3AY8A76iqRwGSvBO4BJgBnF1Va6d0SyRJ2oUNNRxU1VeAr7ThW+k9aTC6zU+A140z/weBD45Rvxi4eAK7KknSbsM3JEqSpA7DgSRJ6jAcSJKkDsOBJEnqMBxIkqQOw4EkSeowHEiSpA7DgSRJ6jAcSJKkDsOBJEnqMBxIkqQOw4EkSeowHEiSpA7DgSRJ6jAcSJKkDsOBJEnqMBxIkqQOw4EkSeowHEiSpA7DgSRJ6jAcSJKkDsOBJEnqMBxIkqQOw4EkSeowHEiSpA7DgSRJ6jAcSJKkDsOBJEnqmPJwkOTgJJcnuSnJ2iQnt/q+SVYnuaX9nNPqSbIiyUiS65Mc1res5a39LUmW99VfkOSGNs+KJJnq7ZQkaVc1jDMHjwDvrqrFwBHAO5IsBk4BLq2qRcClbRzgGGBR+5wEnAG9MAGcBrwQOBw4bUugaG3e0jff0inYLkmSpoUpDwdVdWdVfa0NPwDcDMwDlgErW7OVwLFteBlwbvVcBcxOciBwNLC6qjZW1SZgNbC0Tdu7qq6qqgLO7VuWJEnahqHec5BkAfB84GrggKq6s026CzigDc8D7uibbV2rba2+boy6JEkawNDCQZK9gL8H3lVVm/untb/4awr6cFKSNUnWbNiwYbJXJ0nSLmEo4SDJE+kFg89U1eda+e52SYD2855WXw8c3Df7/FbbWn3+GPXHqKozq2pJVS2ZO3fuzm2UJEnTxDCeVghwFnBzVX2kb9IqYMsTB8uBi/rqJ7SnFo4A7m+XHy4Bjkoyp92IeBRwSZu2OckRbV0n9C1LkiRtw8whrPMlwG8DNyS5rtX+CPgQcEGSE4HvAq9v0y4GXgmMAD8C3gxQVRuTvB+4prV7X1VtbMNvB84B9gS+2D6SJGkAUx4OqupfgPHeO3DkGO0LeMc4yzobOHuM+hrg0J3opiRJuy3fkChJkjoMB5IkqcNwIEmSOgwHkiSpw3AgSZI6DAeSJKnDcCBJkjoMB5IkqcNwIEmSOgwHkiSpw3AgSZI6DAeSJKnDcCBJkjoMB5IkqcNwIEmSOgwHkiSpw3AgSZI6DAeSJKnDcCBJkjoMB5IkqcNwIEmSOgwHkiSpw3AwCTb98OFhd0GSpB1mOJgEbz7nmmF3QZKkHWY4mATfuefBYXdBkqQdZjiQJEkdhgNJktRhOJAkSR3TNhwkWZrkW0lGkpwy7P5IkrSrmJbhIMkM4GPAMcBi4Pgki6dq/T98+JGpWpUkSRNu5rA7MEkOB0aq6laAJOcDy4CbpmLlP6tfDK++6e6pWKUkaZrba4+ZvOhX9puSdU3XcDAPuKNvfB3wwtGNkpwEnARwyCGHTEpH3nLumklZriRp97L4wL25+ORfn5J1TddwMJCqOhM4E2DJkiW1jeYDu/a/v4Jrbt/EvNl7kkzUUiVJu7MnPXHq7gSYruFgPXBw3/j8VpsS++21B0sP/eWpWp0kSRNqWt6QCFwDLEqyMMks4Dhg1ZD7JEnSLmFanjmoqkeSvBO4BJgBnF1Va4fcLUmSdgnTMhwAVNXFwMXD7ockSbua6XpZQZIk7SDDgSRJ6jAcSJKkDsOBJEnqMBxIkqQOw4EkSeowHEiSpI5UTdhXCuzSkmwAvjuBi9wfuHcCl7c7ch9ODPfjznMf7jz34c6b6H34tKqaO9YEw8EkSbKmqpYMux+7MvfhxHA/7jz34c5zH+68qdyHXlaQJEkdhgNJktRhOJg8Zw67A9OA+3BiuB93nvtw57kPd96U7UPvOZAkSR2eOZAkSR2Gg0mQZGmSbyUZSXLKsPvzeJLk4CSXJ7kpydokJ7f6vklWJ7ml/ZzT6kmyou3L65Mc1res5a39LUmWD2ubhiXJjCRfT/L5Nr4wydVtX302yaxW36ONj7TpC/qWcWqrfyvJ0cPZkuFIMjvJhUm+meTmJC/yONw+SX6//X98Y5LzkjzJ43Dbkpyd5J4kN/bVJuzYS/KCJDe0eVYkyXZ3sqr8TOAHmAF8B3g6MAv4BrB42P16vHyAA4HD2vBTgW8Di4E/BU5p9VOAD7fhVwJfBAIcAVzd6vsCt7afc9rwnGFv3xTvyz8A/hb4fBu/ADiuDX8CeFsbfjvwiTZ8HPDZNry4HZ97AAvbcTtj2Ns1hftvJfBf2/AsYLbH4Xbtv3nAbcCefcffmzwOB9p3vwEcBtzYV5uwYw/4amubNu8x29tHzxxMvMOBkaq6taoeBs4Hlg25T48bVXVnVX2tDT8A3EzvH5ll9P6xpv08tg0vA86tnquA2UkOBI4GVlfVxqraBKwGlk7hpgxVkvnAq4BPtvEALwcubE1G78Mt+/ZC4MjWfhlwflU9VFW3ASP0jt9pL8k+9P6BPgugqh6uqvvwONxeM4E9k8wEngzcicfhNlXVlcDGUeUJOfbatL2r6qrqJYVz+5Y1MMPBxJsH3NE3vq7VNEo7rfh84GrggKq6s026CzigDY+3P3f3/fxR4D3Az9r4fsB9VfVIG+/fHz/fV236/a397rwPFwIbgE+1SzOfTPIUPA4HVlXrgT8HvkcvFNwPXIvH4Y6aqGNvXhseXd8uhgMNRZK9gL8H3lVVm/untbTrYzTjSPJq4J6qunbYfdmFzaR3WveMqno+8EN6p3J/zuNw69o18WX0gtZBwFPYvc6aTJrHw7FnOJh464GD+8bnt5qaJE+kFww+U1Wfa+W72+kw2s97Wn28/bk77+eXAK9Jcju9y1YvB/6C3unGma1N//74+b5q0/cBfsDuvQ/XAeuq6uo2fiG9sOBxOLhXALdV1Yaq+inwOXrHpsfhjpmoY299Gx5d3y6Gg4l3DbCo3bE7i96NN6uG3KfHjXaN8Szg5qr6SN+kVcCWu22XAxf11U9od+weAdzfTr1dAhyVZE77C+aoVpv2qurUqppfVQvoHV+XVdUbgcuB17Zmo/fhln372ta+Wv24dhf5QmARvRuZpr2qugu4I8mzWulI4CY8DrfH94Ajkjy5/X+9ZR96HO6YCTn22rTNSY5o/11O6FvW4IZ91+Z0/NC7u/Tb9O66/eNh9+fx9AFeSu902fXAde3zSnrXHi8FbgH+Cdi3tQ/wsbYvbwCW9C3rd+jdvDQCvHnY2zak/fkyfvG0wtPp/aM6AvwdsEerP6mNj7TpT++b/4/bvv0WO3BH8678AZ4HrGnH4j/Su+Pb43D79uGfAN8EbgQ+Te+JA4/Dbe+38+jdp/FTemexTpzIYw9Y0v6bfAf4K9oLD7fn4xsSJUlSh5cVJElSh+FAkiR1GA4kSVKH4UCSJHUYDiRJUofhQNIOS/Jokuv6PhP2LaRJFvR/a52kqTNz200kaVw/rqrnDbsTkiaWZw4kTbgktyf50/ad8l9N8oxWX5Dksva99JcmOaTVD0jyD0m+0T4vbouakeSvk6xN8uUke7b2v5fkprac84e0mdK0ZTiQtDP2HHVZ4Q190+6vqn9P7w1tH221vwRWVtWvAp8BVrT6CuCKqnouve84WNvqi4CPVdVzgPuA32r1U4Dnt+W8dbI2Ttpd+YZESTssyYNVtdcY9duBl1fVre2Ltu6qqv2S3AscWFU/bfU7q2r/JBuA+VX1UN8yFtD7vvpFbfy9wBOr6gNJvgQ8SO+1x/9YVQ9O8qZKuxXPHEiaLDXO8PZ4qG/4UX5xn9Sr6L1v/jDgmr5vAZQ0AQwHkibLG/p+/lsb/ld63yQJ8Ebgn9vwpcDbAJLMSLLPeAtN8gTg4Kq6HHgvva/+fczZC0k7zrQtaWfsmeS6vvEvVdWWxxnnJLme3l//x7fa7wKfSvKHwAbgza1+MnBmkhPpnSF4G71vrRvLDOBvWoAIsKKq7puwLZLkPQeSJl6752BJVd077L5I2n5eVpAkSR2eOZAkSR2eOZAkSR2GA0mS1GE4kCRJHYYDSZLUYTiQJEkdhgNJktTx/wEIcthQasSoBwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test_dataloader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)\n",
        "# result = model(X)\n",
        "# print(y)"
      ],
      "metadata": {
        "id": "3ihpvYBqJ0EA"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.mean(test_batch_losses))"
      ],
      "metadata": {
        "id": "MVmQVs-lKODQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9ec6dea-e8ec-4f0e-e2d4-69c202d2e805"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.09211322\n"
          ]
        }
      ]
    }
  ]
}